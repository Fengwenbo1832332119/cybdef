# scripts/agent/eval_ppo_cskg.py
# -*- coding: utf-8 -*-
"""
è¯„ä¼°è„šæœ¬ï¼šåŠ è½½å·²ç»è®­ç»ƒå¥½çš„ PPO ç­–ç•¥ï¼Œè·‘è‹¥å¹²å›åˆï¼Œç»Ÿè®¡è¡¨ç°

ç”¨æ³•ç¤ºä¾‹ï¼š
    cd C:\cybdef
    conda activate cyborg310
    python scripts/agent/eval_ppo_cskg.py ^
        --model scripts/runs/ppo_cskg/ppo_cskg_xxxx/ac_upd050.pt ^
        --episodes 20
"""

import os
import sys
import argparse
import pathlib
from collections import Counter

import numpy as np
import torch
import torch.nn as nn
from torch.distributions import Categorical

# ===== è·¯å¾„æ³¨å…¥ =====
ROOT = pathlib.Path(__file__).resolve().parents[2]  # C:\cybdef
THIRD = ROOT / "third_party" / "CybORG"

for p in (ROOT, THIRD):
    if str(p) not in sys.path:
        sys.path.insert(0, str(p))

# ===== é¡¹ç›®å†… import =====
try:
    from envs.cyborg_wrapper import CybORGWrapper
except ImportError:
    from scripts.envs.cyborg_wrapper import CybORGWrapper

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# ===== ä¸è®­ç»ƒä¿æŒä¸€è‡´çš„ Actor-Critic ç»“æ„ =====
class ActorCritic(nn.Module):
    def __init__(self, obs_dim: int, act_dim: int, hidden: int = 128):
        super().__init__()
        self.actor = nn.Sequential(
            nn.Linear(obs_dim, hidden),
            nn.Tanh(),
            nn.Linear(hidden, hidden),
            nn.Tanh(),
            nn.Linear(hidden, act_dim),
        )
        self.critic = nn.Sequential(
            nn.Linear(obs_dim, hidden),
            nn.Tanh(),
            nn.Linear(hidden, hidden),
            nn.Tanh(),
            nn.Linear(hidden, 1),
        )

    def forward(self, obs):
        logits = self.actor(obs)
        value = self.critic(obs)
        return logits, value.squeeze(-1)


def to_obs_vector(obs_raw):
    """
    å’Œ train_ppo_cskg_old.py ä¿æŒä¸€è‡´ï¼š
    æ”¯æŒ dict åŒ…è£…ç»“æ„ï¼ˆåŒ…å« obs_vec / obs / observation / state ç­‰ï¼‰
    """
    if isinstance(obs_raw, dict):
        if "obs_vec" in obs_raw:
            obs_raw = obs_raw["obs_vec"]
        else:
            for key in ["obs", "observation", "vector", "state"]:
                if key in obs_raw:
                    obs_raw = obs_raw[key]
                    break

    if isinstance(obs_raw, dict):
        raise TypeError(
            f"æ— æ³•ä» obs å­—å…¸ä¸­æå–å‘é‡ï¼Œè¯·æ£€æŸ¥ keys: {list(obs_raw.keys())}"
        )

    obs_np = np.array(obs_raw, dtype=np.float32).reshape(-1)
    return obs_np


def evaluate(model_path: str, episodes: int = 20, max_steps: int = 100):
    # --- åˆå§‹åŒ–ç¯å¢ƒ ---
    env_yaml = ROOT / "scripts" / "configs" / "env.yaml"
    env = CybORGWrapper(str(env_yaml))

    obs_dim = env.obs_dim
    act_dim = env.action_dim
    action_names = env.action_space.names

    print(f"âœ… è¯„ä¼°ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ: obs_dim={obs_dim}, act_dim={act_dim}")
    print(f"   ä½¿ç”¨æ¨¡å‹: {model_path}")

    # --- åˆå§‹åŒ–å¹¶åŠ è½½æ¨¡å‹ ---
    ac = ActorCritic(obs_dim, act_dim).to(DEVICE)

    ckpt = torch.load(model_path, map_location=DEVICE)
    if isinstance(ckpt, dict) and "model" in ckpt:
        ac.load_state_dict(ckpt["model"])
        print(f"   ğŸ”„ ä» checkpoint å­—å…¸ä¸­åŠ è½½ 'model' æƒé‡")
    else:
        ac.load_state_dict(ckpt)
        print(f"   ğŸ”„ ä»çº¯ state_dict ä¸­åŠ è½½æƒé‡")

    ac.eval()

    # ç»Ÿè®¡
    all_rewards = []
    all_lengths = []
    action_counter = Counter()

    for ep in range(1, episodes + 1):
        obs_raw = env.reset()
        obs = to_obs_vector(obs_raw)

        done = False
        ep_reward = 0.0
        step = 0

        while not done and step < max_steps:
            step += 1
            obs_tensor = torch.from_numpy(obs).to(DEVICE).unsqueeze(0)

            with torch.no_grad():
                logits, _ = ac(obs_tensor)
                logits = logits.squeeze(0)
                dist = Categorical(logits=logits)
                action = dist.sample()

            action_idx = int(action.item())
            action_name = action_names[action_idx]
            action_counter[action_name] += 1

            next_obs_raw, reward_env, done, info = env.step(action_idx)

            ep_reward += float(reward_env)
            obs = to_obs_vector(next_obs_raw)

        all_rewards.append(ep_reward)
        all_lengths.append(step)

        print(f"[EVAL EP {ep:03d}] steps={step:3d}  R_env={ep_reward:.3f}")

    # --- æ±‡æ€»ç»Ÿè®¡ ---
    if len(all_rewards) > 0:
        mean_r = np.mean(all_rewards)
        std_r = np.std(all_rewards)
        mean_len = np.mean(all_lengths)
        print("\n===== è¯„ä¼°ç»“æœæ±‡æ€» =====")
        print(f"  å›åˆæ•°       : {episodes}")
        print(f"  å¹³å‡ EnvReward: {mean_r:.3f} Â± {std_r:.3f}")
        print(f"  å¹³å‡æ­¥é•¿      : {mean_len:.1f}")

    print("\n===== åŠ¨ä½œä½¿ç”¨ç»Ÿè®¡ï¼ˆTop 20ï¼‰ =====")
    for name, cnt in action_counter.most_common(20):
        print(f"  {name:<20s} : {cnt}")

    env.close()


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--model",
        type=str,
        required=True,
        help="æ¨¡å‹ checkpoint è·¯å¾„ï¼Œå¦‚ scripts/runs/ppo_cskg/.../ac_upd050.pt",
    )
    parser.add_argument(
        "--episodes",
        type=int,
        default=20,
        help="è¯„ä¼°å›åˆæ•°ï¼ˆé»˜è®¤ 20ï¼‰",
    )
    parser.add_argument(
        "--max_steps",
        type=int,
        default=100,
        help="æ¯å›åˆæœ€å¤šæ­¥æ•°ï¼ˆé»˜è®¤ 100ï¼‰",
    )

    args = parser.parse_args()

    evaluate(
        model_path=args.model,
        episodes=args.episodes,
        max_steps=args.max_steps,
    )


if __name__ == "__main__":
    main()
