# scripts/agent/train_ppo_cskg_old.py
# -*- coding: utf-8 -*-
"""
PPO + CSKG(KnowledgeBridge) è”åˆè®­ç»ƒè„šæœ¬

- ä½¿ç”¨ CybORGWrapper åŒ…è£…ç¯å¢ƒ
- ä½¿ç”¨ KnowledgeBridge æ³¨å…¥ï¼š
    - åŠ¨ä½œæ©ç ï¼ˆaction_maskï¼‰
    - å…ˆéªŒ logitsï¼ˆprior_logitsï¼‰
    - å¥–åŠ±å¡‘å½¢ï¼ˆreward_shapingï¼‰
- ä¿ç•™ PPO è¶³å¤Ÿè‡ªç”±åº¦ï¼Œè®©ç­–ç•¥è‡ªå·±å­¦ï¼Œè€Œä¸æ˜¯è¢«è§„åˆ™â€œé”æ­»â€
"""

import os, sys, json, time, pathlib, random
from collections import deque
from typing import Any, Dict

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical

# ===== è·¯å¾„æ³¨å…¥ =====
ROOT = pathlib.Path(__file__).resolve().parents[2]  # C:\cybdef
THIRD = ROOT / "third_party" / "CybORG"

for p in (ROOT, THIRD):
    if str(p) not in sys.path:
        sys.path.insert(0, str(p))

# ===== é¡¹ç›®å†… import =====
# env wrapper
try:
    from envs.cyborg_wrapper import CybORGWrapper
except ImportError:
    from scripts.envs.cyborg_wrapper import CybORGWrapper

# CSKG reasoner
from scripts.cskg.reasoner import KnowledgeBridge

import yaml

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def to_serializable(obj):
    """
    é€’å½’æŠŠ numpy ç±»å‹ã€ndarray ç­‰ï¼Œè½¬æˆ Python åŸç”Ÿç±»å‹ï¼Œæ–¹ä¾¿ json.dumps
    """
    import numpy as _np

    if isinstance(obj, (_np.floating,)):
        return float(obj)
    if isinstance(obj, (_np.integer,)):
        return int(obj)
    if isinstance(obj, _np.ndarray):
        return obj.tolist()
    if isinstance(obj, (list, tuple)):
        return [to_serializable(v) for v in obj]
    if isinstance(obj, dict):
        return {k: to_serializable(v) for k, v in obj.items()}
    return obj


# ===== ç®€å• Actor-Critic ç½‘ç»œ =====
class ActorCritic(nn.Module):
    def __init__(self, obs_dim: int, act_dim: int, hidden=128):
        super().__init__()
        self.actor = nn.Sequential(
            nn.Linear(obs_dim, hidden),
            nn.Tanh(),
            nn.Linear(hidden, hidden),
            nn.Tanh(),
            nn.Linear(hidden, act_dim),
        )
        self.critic = nn.Sequential(
            nn.Linear(obs_dim, hidden),
            nn.Tanh(),
            nn.Linear(hidden, hidden),
            nn.Tanh(),
            nn.Linear(hidden, 1),
        )

    def forward(self, obs):
        logits = self.actor(obs)
        value = self.critic(obs)
        return logits, value.squeeze(-1)


# ===== GAE è®¡ç®— =====
def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):
    """
    rewards, values, dones: np.ndarray, shape [T]
    """
    T = len(rewards)
    adv = np.zeros(T, dtype=np.float32)
    gae = 0.0
    for t in reversed(range(T)):
        next_v = values[t + 1] if t + 1 < T else 0.0
        delta = rewards[t] + gamma * next_v * (1.0 - dones[t]) - values[t]
        gae = delta + gamma * lam * (1.0 - dones[t]) * gae
        adv[t] = gae
    return adv


def to_obs_vector(obs_raw: Any) -> np.ndarray:
    """
    æŠŠ env.reset()/step() è¿”å›çš„å„ç§ç»“æ„ï¼Œç»Ÿä¸€è½¬æˆ 1D np.array(float32)

    æ”¯æŒå‡ ç§å¸¸è§å½¢å¼ï¼š
    - dict:
        - åŒ…å« "obs_vec"ï¼ˆBlueTableWrapper é£æ ¼ï¼‰
        - æˆ–åŒ…å« "obs"/"observation"/"vector"/"state" ä¹‹ä¸€
    - å…¶å®ƒï¼šç›´æ¥ np.array(...)
    """
    if isinstance(obs_raw, dict):
        # ä¼˜å…ˆèµ°ä½ åŒ…è£…å¥½çš„ obs_vec
        if "obs_vec" in obs_raw:
            obs_raw = obs_raw["obs_vec"]
        else:
            # é€šç”¨å…œåº•ï¼šå…¼å®¹è€ç‰ˆæœ¬
            for key in ["obs", "observation", "vector", "state"]:
                if key in obs_raw:
                    obs_raw = obs_raw[key]
                    break

    # å¦‚æœè¿˜æ˜¯å­—å…¸ï¼Œè¯´æ˜æ²¡æ³•æ‹¿åˆ°å‘é‡
    if isinstance(obs_raw, dict):
        raise TypeError(
            f"æ— æ³•ä» obs å­—å…¸ä¸­æå–å‘é‡ï¼Œè¯·æ£€æŸ¥ keys: {list(obs_raw.keys())}"
        )

    obs_np = np.array(obs_raw, dtype=np.float32).reshape(-1)
    return obs_np


# ===== ä¸»è®­ç»ƒå‡½æ•° =====
def main():
    global DEVICE

    # --- é…ç½®è·¯å¾„ ---
    ENV_YAML = ROOT / "scripts" / "configs" / "env.yaml"
    CSKG_YAML = ROOT / "scripts" / "configs" / "cskg.yaml"
    SEED_GRAPH = ROOT / "scripts" / "configs" / "seed_graph.json"
    PPO_YAML = ROOT / "scripts" / "configs" / "ppo.yaml"

    RUN_NAME = f"ppo_cskg_{int(time.time())}"
    OUT_DIR = ROOT / "scripts" / "runs" / "ppo_cskg" / RUN_NAME
    os.makedirs(OUT_DIR, exist_ok=True)

    # --- ä» ppo.yaml è¯»å–è¶…å‚ ---
    if PPO_YAML.exists():
        with open(PPO_YAML, "r", encoding="utf-8") as f:
            cfg = yaml.safe_load(f) or {}
    else:
        cfg = {}
        print(f"âš  æœªæ‰¾åˆ° {PPO_YAML}ï¼Œå°†ä½¿ç”¨ä»£ç å†…é»˜è®¤è¶…å‚")

    num_updates = int(cfg.get("num_updates", 100))        # è®­ç»ƒè½®æ•°ï¼ˆåŸ total_episodesï¼‰
    rollout_steps = int(cfg.get("horizon", 256))          # æ¯è½®é‡‡æ ·æ­¥æ•°ï¼ˆåŸ rollout_stepsï¼‰
    ppo_epochs = int(cfg.get("ppo_epochs", 4))
    batch_size = int(cfg.get("mini_batch_size", 64))
    gamma = float(cfg.get("gamma", 0.99))
    lam = float(cfg.get("gae_lambda", 0.95))
    clip_ratio = float(cfg.get("clip_range", 0.2))

    lr_pi = float(cfg.get("pi_lr", 3e-4))
    lr_vf = float(cfg.get("vf_lr", lr_pi))  # ç›®å‰ä»å…±ç”¨ä¸€ä¸ª optimizer
    lr = lr_pi

    entropy_coef = float(cfg.get("entropy_coef", 0.01))
    value_coef = float(cfg.get("value_coef", 0.5))
    rule_coef = float(cfg.get("rule_coef", 0.0))          # ç”¨äºç¼©æ”¾ prior logits
    mask_alpha = float(cfg.get("mask_alpha", 1.0))        # ç›®å‰å…ˆé¢„ç•™ï¼Œä¸å¼ºè¡Œä½¿ç”¨
    max_grad_norm = float(cfg.get("max_grad_norm", 0.5))

    device_cfg = str(cfg.get("device", "cuda")).lower()

    # --- è®¾å¤‡é€‰æ‹©ï¼šä¼˜å…ˆæŒ‰ ppo.yamlï¼Œä½†è¦ä¿è¯å¯ç”¨ ---
    if device_cfg == "cuda" and torch.cuda.is_available():
        DEVICE = torch.device("cuda")
    else:
        DEVICE = torch.device("cpu")

    print(f"ğŸ“‹ PPO é…ç½®æ¥è‡ª: {PPO_YAML}")
    print(f"   num_updates={num_updates}, horizon={rollout_steps}, "
          f"mini_batch_size={batch_size}, ppo_epochs={ppo_epochs}")
    print(f"   gamma={gamma}, gae_lambda={lam}, clip_range={clip_ratio}")
    print(f"   pi_lr={lr_pi}, vf_lr={lr_vf}, entropy_coef={entropy_coef}, value_coef={value_coef}")
    print(f"   rule_coef={rule_coef}, mask_alpha={mask_alpha}, max_grad_norm={max_grad_norm}")
    print(f"   device={DEVICE}")

    # å›ºå®šéšæœºç§å­
    seed = 42
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

    # --- åˆå§‹åŒ–ç¯å¢ƒ ---
    env = CybORGWrapper(str(ENV_YAML))
    obs_dim = env.obs_dim
    act_dim = env.action_dim

    print(f"âœ… PPO+CSKG è®­ç»ƒåˆå§‹åŒ–å®Œæˆ")
    print(f"   obs_dim={obs_dim}, act_dim={act_dim}")
    print(f"   æ—¥å¿—ç›®å½•: {OUT_DIR}")

    # --- åˆå§‹åŒ– CSKG ---
    kb = KnowledgeBridge(
        seed_graph_path=str(SEED_GRAPH),
        cskg_rules_path=str(CSKG_YAML),
        recent_steps=10,
    )

    # --- åˆå§‹åŒ–ç­–ç•¥ç½‘ç»œ ---
    ac = ActorCritic(obs_dim, act_dim).to(DEVICE)
    optimizer = optim.Adam(ac.parameters(), lr=lr)

    # --- å¯è§£é‡Šæ—¥å¿—ï¼šå‰ N æ¬¡ update è¯¦ç»†è®°å½• ---
    explain_log_path = OUT_DIR / "policy_explain_upd1_5.jsonl"
    explain_log_f = open(explain_log_path, "w", encoding="utf-8")

    global_step = 0

    # ===== è®­ç»ƒä¸»å¾ªç¯ï¼šä»¥ num_updates ä¸ºå¤–å±‚è½®æ•° =====
    for upd in range(1, num_updates + 1):
        # env.reset() è¿”å› dict: {"obs_vec", "facts", "raw", ...}
        obs_raw = env.reset()
        if hasattr(kb, "reset_episode"):
            kb.reset_episode()

        # ç¥ç»ç½‘ç»œç”¨çš„å‘é‡è§‚æµ‹
        obs_vec = to_obs_vector(obs_raw)
        # è§„åˆ™å¼•æ“ç”¨çš„è¯­ä¹‰ factsï¼ˆæ¥è‡ª wrapperï¼Œè€Œä¸æ˜¯å†ä» obs_vec åæ¨ï¼‰
        facts = obs_raw.get("facts", {})

        # rollout buffer
        obs_buf = []
        act_buf = []
        logp_buf = []
        rew_buf = []
        val_buf = []
        done_buf = []

        ep_reward_env = 0.0
        ep_reward_total = 0.0

        last_reward_env = 0.0  # å¦‚æœä½ åœ¨ _extract_facts é‡Œæƒ³ç”¨ recent_rewardï¼Œå¯ä»¥ä»è¿™é‡Œå–‚

        # ä¸€æ¬¡ update å†…é‡‡æ · rollout_steps æ­¥ï¼ˆå¯èƒ½è·¨ episodeï¼Œä¸­é€” done å°±é‡ç½®ï¼‰
        steps_collected = 0
        while steps_collected < rollout_steps:
            global_step += 1

            # === ç­–ç•¥ç½‘ç»œå‰å‘ ===
            obs_tensor = torch.from_numpy(obs_vec).to(DEVICE).unsqueeze(0)
            logits, value = ac(obs_tensor)  # [1, act_dim], [1]
            logits = logits.squeeze(0)  # [act_dim]
            value = value.squeeze(0)  # scalar

            action_names = env.action_space.names

            # === CSKG: å…ˆç”¨ facts æ›´æ–°å†…éƒ¨çŠ¶æ€ï¼ˆå¯é€‰ï¼‰===
            if hasattr(kb, "update_from_facts"):
                kb.update_from_facts(facts)

            # === ä» KB è·å–å…ˆéªŒä¸æ©ç  ===
            prior_np = kb.prior_logits(facts, action_names)
            # æœ‰äº›ç‰ˆæœ¬å¯èƒ½è¿”å› (prior, debug_info)
            if isinstance(prior_np, tuple):
                prior_np = prior_np[0]
            prior_np = np.array(prior_np, dtype=np.float32)

            mask_res = kb.query_action_mask(facts, action_names)
            if isinstance(mask_res, tuple):
                rule_mask_np = np.array(mask_res[0], dtype=np.float32)
            else:
                rule_mask_np = np.array(mask_res, dtype=np.float32)

            # ç¯å¢ƒè‡ªå¸¦åˆæ³•æ©ç 
            try:
                legal_mask_np = env._current_legal_mask().astype(np.float32)
            except Exception:
                # å¦‚æœæ²¡æœ‰è¯¥æ¥å£ï¼Œå°±å‡è®¾å…¨éƒ¨åˆæ³•
                legal_mask_np = np.ones(act_dim, dtype=np.float32)

            if rule_mask_np.shape[0] != act_dim:
                raise ValueError(f"rule_mask ç»´åº¦å¼‚å¸¸: {rule_mask_np.shape[0]} vs act_dim={act_dim}")
            if prior_np.shape[0] != act_dim:
                raise ValueError(f"prior ç»´åº¦å¼‚å¸¸: {prior_np.shape[0]} vs act_dim={act_dim}")
            if legal_mask_np.shape[0] != act_dim:
                raise ValueError(f"legal_mask ç»´åº¦å¼‚å¸¸: {legal_mask_np.shape[0]} vs act_dim={act_dim}")

            # èåˆæ©ç ï¼ˆç¯å¢ƒ Ã— è§„åˆ™ï¼‰
            combined_mask_np = (legal_mask_np * rule_mask_np).astype(np.float32)
            if combined_mask_np.sum() <= 0:
                # æç«¯æƒ…å†µï¼šå…¨ 0ï¼Œå°±æ”¾å¼€ä¸€ä¸ª no-opï¼ˆSleep=0ï¼‰
                combined_mask_np[0] = 1.0

            # ==== logits + prior + mask ====
            logits = logits.clone()
            prior = torch.from_numpy(prior_np).to(DEVICE)

            # èåˆå…ˆéªŒï¼ˆå¸¦ rule_coefï¼‰
            if rule_coef != 0.0:
                logits = logits + rule_coef * prior
            else:
                logits = logits + prior

            # æ©ç ï¼šcombined_mask == 0 çš„åŠ¨ä½œè§†ä¸ºä¸å¯é€‰
            combined_mask_t = torch.from_numpy(combined_mask_np).to(DEVICE)
            logits[combined_mask_t == 0] = -1e9

            dist = Categorical(logits=logits)
            action = dist.sample()
            logp = dist.log_prob(action)

            action_idx = int(action.item())
            action_name = action_names[action_idx]

            # === ä¸ç¯å¢ƒäº¤äº’ ===
            next_obs_raw, reward_env, done, info = env.step(action_idx)

            # === ä¿®æ­£ï¼šæ­£ç¡®çš„å¥–åŠ±å¡‘å½¢ ===
            env_reward = float(reward_env)  # ç¯å¢ƒåŸå§‹å¥–åŠ±ï¼ˆçœŸå®æ€§èƒ½ï¼‰

            # CSKGå¥–åŠ±å¡‘å½¢ï¼ˆåŸºäºç¯å¢ƒå¥–åŠ±ï¼‰
            if hasattr(kb, "step_update"):
                shaped_reward = kb.step_update(facts, action_name, env_reward)
            else:
                shaped_reward = env_reward

            # å…³é”®è®¾è®¡ï¼šè®­ç»ƒç”¨å¡‘å½¢å¥–åŠ±ï¼Œè¯„ä¼°ç”¨ç¯å¢ƒå¥–åŠ±
            if env.mode == "train":
                r_total = shaped_reward  # PPOç”¨CSKGæŒ‡å¯¼çš„è®­ç»ƒä¿¡å·
            else:
                r_total = env_reward  # è¯„ä¼°æ—¶ç”¨çœŸå®ç¯å¢ƒå¥–åŠ±

            # === ä¿®æ­£ï¼šæ­£ç¡®çš„KBçŠ¶æ€æ›´æ–° ===
            next_facts = next_obs_raw.get("facts", {})
            # å¦‚æœæœ‰ä¸“é—¨çš„KBçŠ¶æ€æ›´æ–°æ–¹æ³•ï¼Œåœ¨è¿™é‡Œè°ƒç”¨ï¼ˆä½†step_updateå¯èƒ½å·²ç»å¤„ç†äº†ï¼‰
            # æ³¨æ„ï¼šæˆ‘ä»¬ä¸å†é‡å¤è°ƒç”¨step_updateï¼Œå› ä¸ºå®ƒå·²ç»è¿”å›äº†å¡‘å½¢å¥–åŠ±

            last_reward_env = env_reward  # ç”¨äº_extract_factsçš„recent_reward

            # ==== å†™å…¥ rollout bufferï¼ˆç”¨ r_total æ¥è®­ç»ƒ PPOï¼‰ ====
            obs_buf.append(obs_vec.copy())
            act_buf.append(action_idx)
            logp_buf.append(float(logp.item()))
            rew_buf.append(float(r_total))  # PPOç”¨è®­ç»ƒä¿¡å·
            val_buf.append(float(value.item()))
            done_buf.append(float(done))

            # åˆ†åˆ«è®°å½•ä¸¤ç§å¥–åŠ±ç”¨äºåˆ†æ
            ep_reward_env += env_reward  # çœŸå®ç¯å¢ƒè¡¨ç°
            ep_reward_total += r_total  # å®é™…è®­ç»ƒä¿¡å·

            steps_collected += 1

            # === å¯è§£é‡Šæ—¥å¿—ï¼šå‰ 5 æ¬¡ update è¯¦ç»†è®°å½• ===
            if upd <= 5:
                top_idx = np.argsort(prior_np)[-3:][::-1]
                top_prior = [
                    [action_names[i], float(prior_np[i])]
                    for i in top_idx
                ]
                try:
                    explain = kb.explain_decision(facts, action_names)
                except Exception:
                    explain = {}

                explain_rec = {
                    "update": upd,
                    "step": steps_collected,
                    "global_step": global_step,
                    "action_idx": action_idx,
                    "action_name": action_name,
                    "reward_env": float(env_reward),  # è®°å½•ç¯å¢ƒå¥–åŠ±
                    "reward_shaped": float(shaped_reward),  # è®°å½•å¡‘å½¢å¥–åŠ±
                    "reward_total": float(r_total),  # è®°å½•å®é™…è®­ç»ƒä¿¡å·
                    "legal_mask_sum": float(legal_mask_np.sum()),
                    "rule_mask_sum": float(rule_mask_np.sum()),
                    "combined_mask_sum": float(combined_mask_np.sum()),
                    "top_prior": top_prior,
                    "fact": facts,
                    "explain": explain,
                }
                explain_log_f.write(
                    json.dumps(to_serializable(explain_rec), ensure_ascii=False) + "\n"
                )

            # === å‡†å¤‡ä¸‹ä¸€æ­¥ ===
            obs_vec = to_obs_vector(next_obs_raw)  # ä»…ç”¨äº NN
            facts = next_facts  # ä¸‹ä¸€æ­¥è§„åˆ™ä½¿ç”¨çš„ facts

            # å¦‚æœ episode ç»“æŸï¼Œé‡ç½®ç¯å¢ƒ + KBï¼Œä½†ç»§ç»­æœ¬æ¬¡ update ç›´åˆ°æ”¶æ»¡ horizon
            if done and steps_collected < rollout_steps:
                obs_raw = env.reset()
                if hasattr(kb, "reset_episode"):
                    kb.reset_episode()
                obs_vec = to_obs_vector(obs_raw)
                facts = obs_raw.get("facts", {})
                last_reward_env = 0.0

        # ===== ä¸€æ¬¡ update ç»“æŸï¼šPPO æ›´æ–° =====
        T = len(rew_buf)
        if T == 0:
            continue

        rewards = np.array(rew_buf, dtype=np.float32)
        values = np.array(val_buf, dtype=np.float32)
        dones = np.array(done_buf, dtype=np.float32)

        # æœ«å€¼ bootstrap = 0ï¼ˆè¿™é‡Œç®€å•å¤„ç†ï¼‰
        values_ext = np.concatenate([values, np.array([0.0], dtype=np.float32)], axis=0)

        adv = compute_gae(rewards, values_ext, dones, gamma=gamma, lam=lam)
        returns = adv + values

        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        adv = (adv - adv.mean()) / (adv.std() + 1e-8)

        # è½¬ tensor
        obs_tensor = torch.from_numpy(np.array(obs_buf, dtype=np.float32)).to(DEVICE)
        act_tensor = torch.from_numpy(np.array(act_buf, dtype=np.int64)).to(DEVICE)
        logp_old_tensor = torch.from_numpy(np.array(logp_buf, dtype=np.float32)).to(DEVICE)
        adv_tensor = torch.from_numpy(adv).to(DEVICE)
        ret_tensor = torch.from_numpy(returns).to(DEVICE)

        # å¤š epoch æ‰“ä¹±è®­ç»ƒ
        num_samples = T
        idxs = np.arange(num_samples)

        for _ in range(ppo_epochs):
            np.random.shuffle(idxs)
            for start in range(0, num_samples, batch_size):
                end = start + batch_size
                batch_idx = idxs[start:end]

                b_obs = obs_tensor[batch_idx]
                b_act = act_tensor[batch_idx]
                b_logp_old = logp_old_tensor[batch_idx]
                b_adv = adv_tensor[batch_idx]
                b_ret = ret_tensor[batch_idx]

                logits, values_pred = ac(b_obs)
                dist = Categorical(logits=logits)
                logp = dist.log_prob(b_act)
                entropy = dist.entropy().mean()

                ratio = torch.exp(logp - b_logp_old)
                surr1 = ratio * b_adv
                surr2 = torch.clamp(ratio, 1.0 - clip_ratio, 1.0 + clip_ratio) * b_adv
                policy_loss = -torch.min(surr1, surr2).mean()

                value_loss = ((values_pred - b_ret) ** 2).mean()

                loss = policy_loss + value_coef * value_loss - entropy_coef * entropy

                optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(ac.parameters(), max_grad_norm)
                optimizer.step()

        # æ‰“å°æ—¶æ˜¾ç¤ºä¸¤ç§å¥–åŠ±
        print(
            f"[UPD {upd:03d}] steps={T:4d}  "
            f"Env_R={ep_reward_env:.3f}  Shaped_R={ep_reward_total:.3f}"
        )

        # ç®€å•ä¿å­˜ checkpointï¼ˆæ¯ 25 æ¬¡ updateï¼‰
        if upd % 25 == 0:
            ckpt_path = OUT_DIR / f"ac_upd{upd:03d}.pt"
            torch.save(
                {
                    "model": ac.state_dict(),
                    "optimizer": optimizer.state_dict(),
                    "update": upd,
                    "global_step": global_step,
                },
                ckpt_path,
            )
            print(f" ğŸ’¾ å·²ä¿å­˜ checkpoint: {ckpt_path}")

    explain_log_f.close()
    env.close()
    print("âœ… è®­ç»ƒç»“æŸ")


if __name__ == "__main__":
    main()
