# scripts/agent/train_ppo_cskg.py
# -*- coding: utf-8 -*-
"""
PPO + CSKG(KnowledgeBridge) è”åˆè®­ç»ƒè„šæœ¬

- ä½¿ç”¨ CybORGWrapper åŒ…è£…ç¯å¢ƒ
- ä½¿ç”¨ KnowledgeBridge æ³¨å…¥ï¼š
    - åŠ¨ä½œæ©ç ï¼ˆaction_maskï¼‰
    - å…ˆéªŒ logitsï¼ˆprior_logitsï¼‰
    - å¥–åŠ±å¡‘å½¢ï¼ˆreward_shapingï¼‰
- ä¿ç•™ PPO è¶³å¤Ÿè‡ªç”±åº¦ï¼Œè®©ç­–ç•¥è‡ªå·±å­¦ï¼Œè€Œä¸æ˜¯è¢«è§„åˆ™â€œé”æ­»â€
"""

import os
import sys
import json
import time
import pathlib
import random
import argparse
from typing import Any, Dict, Tuple

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical

# ===== è·¯å¾„æ³¨å…¥ =====
ROOT = pathlib.Path(__file__).resolve().parents[2]  # C:\cybdef
THIRD = ROOT / "third_party" / "CybORG"

for p in (ROOT, THIRD):
    if str(p) not in sys.path:
        sys.path.insert(0, str(p))

# ===== é¡¹ç›®å†… import =====
# env wrapper
try:
    from envs.cyborg_wrapper import CybORGWrapper
except ImportError:
    from scripts.envs.cyborg_wrapper import CybORGWrapper

# CSKG reasoner
from scripts.cskg.reasoner import KnowledgeBridge

import yaml

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def to_serializable(obj):
    """
    é€’å½’æŠŠ numpy ç±»å‹ã€ndarray ç­‰ï¼Œè½¬æˆ Python åŸç”Ÿç±»å‹ï¼Œæ–¹ä¾¿ json.dumps
    """
    import numpy as _np

    if isinstance(obj, (_np.floating,)):
        return float(obj)
    if isinstance(obj, (_np.integer,)):
        return int(obj)
    if isinstance(obj, _np.ndarray):
        return obj.tolist()
    if isinstance(obj, (list, tuple)):
        return [to_serializable(v) for v in obj]
    if isinstance(obj, dict):
        return {k: to_serializable(v) for k, v in obj.items()}
    return obj


def load_yaml(path: pathlib.Path) -> Dict[str, Any]:
    if not path.exists():
        return {}
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}


def build_paths(cfg: Dict[str, Any]) -> Dict[str, pathlib.Path]:
    paths_cfg = cfg.get("paths", {})
    return {
        "env": ROOT / paths_cfg.get("env_config", "scripts/configs/env.yaml"),
        "ppo": ROOT / paths_cfg.get("ppo_config", "scripts/configs/ppo.yaml"),
        "cskg": ROOT / paths_cfg.get("cskg_config", "scripts/configs/cskg.yaml"),
        "seed_graph": ROOT / paths_cfg.get("seed_graph", "scripts/configs/seed_graph.json"),
    }

def dump_run_metadata(out_dir: pathlib.Path, exp_cfg: Dict[str, Any], ppo_cfg: Dict[str, Any]) -> None:
    """å°†å®éªŒä¸è¶…å‚å¿«ç…§å†™å…¥æ—¥å¿—ç›®å½•ï¼Œæ–¹ä¾¿å¤ç°ã€‚"""

    out_dir.mkdir(parents=True, exist_ok=True)
    with open(out_dir / "exp_config_snapshot.json", "w", encoding="utf-8") as f:
        json.dump(to_serializable(exp_cfg), f, ensure_ascii=False, indent=2)

    with open(out_dir / "ppo_config_snapshot.json", "w", encoding="utf-8") as f:
        json.dump(to_serializable(ppo_cfg), f, ensure_ascii=False, indent=2)


# ===== ç®€å• Actor-Critic ç½‘ç»œ =====
class ActorCritic(nn.Module):
    def __init__(self, obs_dim: int, act_dim: int, hidden=128):
        super().__init__()
        self.actor = nn.Sequential(
            nn.Linear(obs_dim, hidden),
            nn.Tanh(),
            nn.Linear(hidden, hidden),
            nn.Tanh(),
            nn.Linear(hidden, act_dim),
        )
        self.critic = nn.Sequential(
            nn.Linear(obs_dim, hidden),
            nn.Tanh(),
            nn.Linear(hidden, hidden),
            nn.Tanh(),
            nn.Linear(hidden, 1),
        )

    def forward(self, obs):
        logits = self.actor(obs)
        value = self.critic(obs)
        return logits, value.squeeze(-1)


# ===== GAE è®¡ç®— =====
def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):
    """
    rewards, values, dones: np.ndarray, shape [T]
    """
    T = len(rewards)
    adv = np.zeros(T, dtype=np.float32)
    gae = 0.0
    for t in reversed(range(T)):
        next_v = values[t + 1] if t + 1 < T else 0.0
        delta = rewards[t] + gamma * next_v * (1.0 - dones[t]) - values[t]
        gae = delta + gamma * lam * (1.0 - dones[t]) * gae
        adv[t] = gae
    return adv


def to_obs_vector(obs_raw: Any) -> np.ndarray:
    """
    æŠŠ env.reset()/step() è¿”å›çš„å„ç§ç»“æ„ï¼Œç»Ÿä¸€è½¬æˆ 1D np.array(float32)

    æ”¯æŒå‡ ç§å¸¸è§å½¢å¼ï¼š
    - dict:
        - åŒ…å« "obs_vec"ï¼ˆBlueTableWrapper é£æ ¼ï¼‰
        - æˆ–åŒ…å« "obs"/"observation"/"vector"/"state" ä¹‹ä¸€
    - å…¶å®ƒï¼šç›´æ¥ np.array(...)
    """
    if isinstance(obs_raw, dict):
        # ä¼˜å…ˆèµ°ä½ åŒ…è£…å¥½çš„ obs_vec
        if "obs_vec" in obs_raw:
            obs_raw = obs_raw["obs_vec"]
        else:
            # é€šç”¨å…œåº•ï¼šå…¼å®¹è€ç‰ˆæœ¬
            for key in ["obs", "observation", "vector", "state"]:
                if key in obs_raw:
                    obs_raw = obs_raw[key]
                    break

    # å¦‚æœè¿˜æ˜¯å­—å…¸ï¼Œè¯´æ˜æ²¡æ³•æ‹¿åˆ°å‘é‡
    if isinstance(obs_raw, dict):
        raise TypeError(
            f"æ— æ³•ä» obs å­—å…¸ä¸­æå–å‘é‡ï¼Œè¯·æ£€æŸ¥ keys: {list(obs_raw.keys())}"
        )

    # ä¸æ—©æœŸç‰ˆæœ¬çš„ to_obs_vector ä¿æŒä¸€è‡´ï¼šæ— è®ºè¾“å…¥æ˜¯ list/np.ndarray/æ ‡é‡éƒ½å±•å¼€ä¸º 1D float32
    obs_np = np.array(obs_raw, dtype=np.float32).reshape(-1)
    return obs_np

def select_action(
    ac: ActorCritic,
    kb: KnowledgeBridge | None,
    env: CybORGWrapper,
    obs_vec: np.ndarray,
    facts: Dict[str, Any],
    action_names: list[str],
    rule_coef: float,
) -> Tuple[int, float, float, Dict[str, Any]]:
    """å‰å‘ã€èåˆå…ˆéªŒ/æ©ç å¹¶é‡‡æ ·åŠ¨ä½œï¼Œè¿”å› action_idxã€logpã€valueã€è°ƒè¯•ä¿¡æ¯ã€‚"""

    obs_tensor = torch.from_numpy(obs_vec).to(DEVICE).unsqueeze(0)
    logits, value = ac(obs_tensor)  # [1, act_dim], [1]
    logits = logits.squeeze(0)  # [act_dim]
    value = value.squeeze(0)

    # === CSKG: å…ˆç”¨ facts æ›´æ–°å†…éƒ¨çŠ¶æ€ï¼ˆå¯é€‰ï¼‰===
    if kb and hasattr(kb, "update_from_facts"):
        kb.update_from_facts(facts)

    # === ä» KB è·å–å…ˆéªŒä¸æ©ç  ===
    if kb:
        prior_np = kb.prior_logits(facts, action_names)
        if isinstance(prior_np, tuple):
            prior_np = prior_np[0]
        prior_np = np.array(prior_np, dtype=np.float32)

        mask_res = kb.query_action_mask(facts, action_names)
        if isinstance(mask_res, tuple):
            rule_mask_np = np.array(mask_res[0], dtype=np.float32)
        else:
            rule_mask_np = np.array(mask_res, dtype=np.float32)
    else:
        prior_np = np.zeros(len(action_names), dtype=np.float32)
        rule_mask_np = np.ones(len(action_names), dtype=np.float32)

    # ç¯å¢ƒè‡ªå¸¦åˆæ³•æ©ç 
    try:
        legal_mask_np = env._current_legal_mask().astype(np.float32)
    except Exception:
        legal_mask_np = np.ones(len(action_names), dtype=np.float32)

    # èåˆæ©ç ï¼ˆç¯å¢ƒ Ã— è§„åˆ™ï¼‰
    combined_mask_np = (legal_mask_np * rule_mask_np).astype(np.float32)
    if combined_mask_np.sum() <= 0:
        combined_mask_np[0] = 1.0  # é¿å…æ­»é”

    logits = logits.clone()
    prior = torch.from_numpy(prior_np).to(DEVICE)
    logits = logits + (rule_coef * prior if rule_coef != 0.0 else prior)

    combined_mask_t = torch.from_numpy(combined_mask_np).to(DEVICE)
    logits[combined_mask_t == 0] = -1e9

    dist = Categorical(logits=logits)
    action = dist.sample()
    logp = dist.log_prob(action)

    debug_info = {
        "prior_np": prior_np,
        "rule_mask_np": rule_mask_np,
        "legal_mask_np": legal_mask_np,
        "combined_mask_np": combined_mask_np,
    }

    return int(action.item()), float(logp.item()), float(value.item()), debug_info


# ===== ä¸»è®­ç»ƒå‡½æ•° =====
def main(config_path: str | None = None):
    global DEVICE

    if config_path is None:
        parser = argparse.ArgumentParser(description="PPO è®­ç»ƒå…¥å£")
        parser.add_argument(
            "--config",
            type=str,
            default=str(ROOT / "scripts" / "configs" / "b1.yaml"),
            help="å®éªŒé…ç½®æ–‡ä»¶ï¼ˆB0/B1/B2ï¼‰",
        )
        args = parser.parse_args()
        config_path = args.config

    exp_cfg = load_yaml(pathlib.Path(config_path))
    if not exp_cfg:
        raise FileNotFoundError(f"æ— æ³•åŠ è½½é…ç½®: {config_path}")

    paths = build_paths(exp_cfg)

    exp_meta = exp_cfg.get("experiment", {})
    features = exp_cfg.get("features", {})
    logging_cfg = exp_cfg.get("logging", {})

    run_prefix = logging_cfg.get("run_prefix", "ppo_cskg")
    run_id = exp_meta.get("id", "exp")

    RUN_NAME = f"{run_prefix.lower()}_{run_id.lower()}_{int(time.time())}"
    OUT_DIR = ROOT / "scripts" / "runs" / run_prefix / RUN_NAME

    os.makedirs(OUT_DIR, exist_ok=True)

    print(f"ğŸ”§ è¿è¡Œ ID: {RUN_NAME}")
    print(
        "   ç‰¹æ€§: CSKG={}  RAG_explain={}".format(
            features.get("enable_cskg", True), features.get("enable_rag_explain", False)
        )
    )

    # --- ä» ppo.yaml è¯»å–è¶…å‚ ---
    ppo_yaml = paths["ppo"]
    if ppo_yaml.exists():
        with open(ppo_yaml, "r", encoding="utf-8") as f:
            cfg = yaml.safe_load(f) or {}
    else:
        cfg = {}
        print(f"âš  æœªæ‰¾åˆ° {ppo_yaml}ï¼Œå°†ä½¿ç”¨ä»£ç å†…é»˜è®¤è¶…å‚")

    num_updates = int(cfg.get("num_updates", 100))        # è®­ç»ƒè½®æ•°ï¼ˆåŸ total_episodesï¼‰
    rollout_steps = int(cfg.get("horizon", 256))          # æ¯è½®é‡‡æ ·æ­¥æ•°ï¼ˆåŸ rollout_stepsï¼‰
    ppo_epochs = int(cfg.get("ppo_epochs", 4))
    batch_size = int(cfg.get("mini_batch_size", 64))
    gamma = float(cfg.get("gamma", 0.99))
    lam = float(cfg.get("gae_lambda", 0.95))
    clip_ratio = float(cfg.get("clip_range", 0.2))

    lr_pi = float(cfg.get("pi_lr", 3e-4))
    lr_vf = float(cfg.get("vf_lr", lr_pi))  # ç›®å‰ä»å…±ç”¨ä¸€ä¸ª optimizer
    lr = lr_pi

    entropy_coef = float(cfg.get("entropy_coef", 0.01))
    value_coef = float(cfg.get("value_coef", 0.5))
    rule_coef = float(cfg.get("rule_coef", 0.0))          # ç”¨äºç¼©æ”¾ prior logits
    max_grad_norm = float(cfg.get("max_grad_norm", 0.5))

    device_cfg = str(cfg.get("device", "cuda")).lower()

    # --- è®¾å¤‡é€‰æ‹©ï¼šä¼˜å…ˆæŒ‰ ppo.yamlï¼Œä½†è¦ä¿è¯å¯ç”¨ ---
    if device_cfg == "cuda" and torch.cuda.is_available():
        DEVICE = torch.device("cuda")
    else:
        DEVICE = torch.device("cpu")

    dump_run_metadata(OUT_DIR, exp_cfg, cfg)

    print(f"ğŸ“‹ å®éªŒé…ç½®: {config_path}")
    print(f"ğŸ“‹ PPO é…ç½®æ¥è‡ª: {ppo_yaml}")
    print(f"   num_updates={num_updates}, horizon={rollout_steps}, "
          f"mini_batch_size={batch_size}, ppo_epochs={ppo_epochs}")
    print(f"   gamma={gamma}, gae_lambda={lam}, clip_range={clip_ratio}")
    print(f"   pi_lr={lr_pi}, vf_lr={lr_vf}, entropy_coef={entropy_coef}, value_coef={value_coef}")
    print(f"   rule_coef={rule_coef}, max_grad_norm={max_grad_norm}")
    print(f"   device={DEVICE}")

    # å›ºå®šéšæœºç§å­
    seed = int(exp_meta.get("seed", 42))
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)

    # --- åˆå§‹åŒ–ç¯å¢ƒ ---
    env = CybORGWrapper(str(paths["env"]))
    obs_dim = env.obs_dim
    act_dim = env.action_dim

    print(f"âœ… PPO è®­ç»ƒåˆå§‹åŒ–å®Œæˆ")
    print(f"   obs_dim={obs_dim}, act_dim={act_dim}")
    print(f"   æ—¥å¿—ç›®å½•: {OUT_DIR}")

    enable_cskg = bool(features.get("enable_cskg", True))
    enable_rag = bool(features.get("enable_rag_explain", False))
    kb = None
    if enable_cskg:
        kb = KnowledgeBridge(
            seed_graph_path=str(paths["seed_graph"]),
            cskg_rules_path=str(paths["cskg"]),
            recent_steps=10,
        )
    elif enable_rag:
        print("âš  RAG è§£é‡Šè¢«å¯ç”¨ä½† CSKG å…³é—­ï¼Œå°†è·³è¿‡ KB ç›¸å…³æ—¥å¿—")

    # --- åˆå§‹åŒ–ç­–ç•¥ç½‘ç»œ ---
    ac = ActorCritic(obs_dim, act_dim).to(DEVICE)
    optimizer = optim.Adam(ac.parameters(), lr=lr)

    # --- å¯è§£é‡Šæ—¥å¿—ï¼šå‰ N æ¬¡ update è¯¦ç»†è®°å½• ---
    explain_log_path = OUT_DIR / "policy_explain_upd1_5.jsonl"
    explain_log_f = None
    if enable_cskg or enable_rag:
        explain_log_f = open(explain_log_path, "w", encoding="utf-8")

    global_step = 0

    # ===== è®­ç»ƒä¸»å¾ªç¯ï¼šä»¥ num_updates ä¸ºå¤–å±‚è½®æ•° =====
    for upd in range(1, num_updates + 1):
        # env.reset() è¿”å› dict: {"obs_vec", "facts", "raw", ...}
        obs_raw = env.reset()
        if kb and hasattr(kb, "reset_episode"):
            kb.reset_episode()

        # ç¥ç»ç½‘ç»œç”¨çš„å‘é‡è§‚æµ‹
        obs_vec = to_obs_vector(obs_raw)
        # è§„åˆ™å¼•æ“ç”¨çš„è¯­ä¹‰ factsï¼ˆæ¥è‡ª wrapperï¼Œè€Œä¸æ˜¯å†ä» obs_vec åæ¨ï¼‰
        facts = obs_raw.get("facts", {})

        # rollout buffer
        obs_buf = []
        act_buf = []
        logp_buf = []
        rew_buf = []
        val_buf = []
        done_buf = []

        ep_reward_env = 0.0
        ep_reward_total = 0.0


        # ä¸€æ¬¡ update å†…é‡‡æ · rollout_steps æ­¥ï¼ˆå¯èƒ½è·¨ episodeï¼Œä¸­é€” done å°±é‡ç½®ï¼‰
        steps_collected = 0
        while steps_collected < rollout_steps:
            global_step += 1

            action_names = env.action_space.names

            action_idx, logp, value, debug_info = select_action(
                ac, kb, env, obs_vec, facts, action_names, rule_coef
            )
            action_name = action_names[action_idx]

            # === ä¸ç¯å¢ƒäº¤äº’ ===
            next_obs_raw, reward_env, done, info = env.step(action_idx)

            # === ä¿®æ­£ï¼šæ­£ç¡®çš„å¥–åŠ±å¡‘å½¢ ===
            env_reward = float(reward_env)  # ç¯å¢ƒåŸå§‹å¥–åŠ±ï¼ˆçœŸå®æ€§èƒ½ï¼‰

            # CSKGå¥–åŠ±å¡‘å½¢ï¼ˆåŸºäºç¯å¢ƒå¥–åŠ±ï¼‰
            if kb and hasattr(kb, "step_update"):
                shaped_reward = kb.step_update(facts, action_name, env_reward)
            else:
                shaped_reward = env_reward

            # å…³é”®è®¾è®¡ï¼šè®­ç»ƒç”¨å¡‘å½¢å¥–åŠ±ï¼Œè¯„ä¼°ç”¨ç¯å¢ƒå¥–åŠ±
            if env.mode == "train":
                r_total = shaped_reward  # PPOç”¨CSKGæŒ‡å¯¼çš„è®­ç»ƒä¿¡å·
            else:
                r_total = env_reward  # è¯„ä¼°æ—¶ç”¨çœŸå®ç¯å¢ƒå¥–åŠ±

            # === ä¿®æ­£ï¼šæ­£ç¡®çš„KBçŠ¶æ€æ›´æ–° ===
            next_facts = next_obs_raw.get("facts", {})
            # å¦‚æœæœ‰ä¸“é—¨çš„KBçŠ¶æ€æ›´æ–°æ–¹æ³•ï¼Œåœ¨è¿™é‡Œè°ƒç”¨ï¼ˆä½†step_updateå¯èƒ½å·²ç»å¤„ç†äº†ï¼‰
            # æ³¨æ„ï¼šæˆ‘ä»¬ä¸å†é‡å¤è°ƒç”¨step_updateï¼Œå› ä¸ºå®ƒå·²ç»è¿”å›äº†å¡‘å½¢å¥–åŠ±

            last_reward_env = env_reward  # ç”¨äº_extract_factsçš„recent_reward

            # ==== å†™å…¥ rollout bufferï¼ˆç”¨ r_total æ¥è®­ç»ƒ PPOï¼‰ ====
            obs_buf.append(obs_vec.copy())
            act_buf.append(action_idx)
            logp_buf.append(logp)
            rew_buf.append(float(r_total))  # PPOç”¨è®­ç»ƒä¿¡å·
            val_buf.append(value)
            done_buf.append(float(done))

            # åˆ†åˆ«è®°å½•ä¸¤ç§å¥–åŠ±ç”¨äºåˆ†æ
            ep_reward_env += env_reward  # çœŸå®ç¯å¢ƒè¡¨ç°
            ep_reward_total += r_total  # å®é™…è®­ç»ƒä¿¡å·

            steps_collected += 1

            # === å¯è§£é‡Šæ—¥å¿—ï¼šå‰ 5 æ¬¡ update è¯¦ç»†è®°å½• ===
            if upd <= 5 and (enable_cskg or enable_rag):
                prior_np = debug_info["prior_np"]
                top_idx = np.argsort(prior_np)[-3:][::-1]
                top_prior = [[action_names[i], float(prior_np[i])] for i in top_idx]
                try:
                    explain = kb.explain_decision(facts, action_names) if kb else {}
                except Exception:
                    explain = {}

                explain_rec = {
                    "update": upd,
                    "step": steps_collected,
                    "global_step": global_step,
                    "action_idx": action_idx,
                    "action_name": action_name,
                    "reward_env": float(env_reward),  # è®°å½•ç¯å¢ƒå¥–åŠ±
                    "reward_shaped": float(shaped_reward),  # è®°å½•å¡‘å½¢å¥–åŠ±
                    "reward_total": float(r_total),  # è®°å½•å®é™…è®­ç»ƒä¿¡å·
                    "legal_mask_sum": float(debug_info["legal_mask_np"].sum()),
                    "rule_mask_sum": float(debug_info["rule_mask_np"].sum()),
                    "combined_mask_sum": float(debug_info["combined_mask_np"].sum()),
                    "top_prior": top_prior,
                    "fact": facts,
                    "explain": explain,
                }
                explain_log_f.write(
                    json.dumps(to_serializable(explain_rec), ensure_ascii=False) + "\n"
                )

            # === å‡†å¤‡ä¸‹ä¸€æ­¥ ===
            obs_vec = to_obs_vector(next_obs_raw)  # ä»…ç”¨äº NN
            facts = next_facts  # ä¸‹ä¸€æ­¥è§„åˆ™ä½¿ç”¨çš„ facts

            # å¦‚æœ episode ç»“æŸï¼Œé‡ç½®ç¯å¢ƒ + KBï¼Œä½†ç»§ç»­æœ¬æ¬¡ update ç›´åˆ°æ”¶æ»¡ horizon
            if done and steps_collected < rollout_steps:
                obs_raw = env.reset()
                if hasattr(kb, "reset_episode"):
                    kb.reset_episode()
                obs_vec = to_obs_vector(obs_raw)
                facts = obs_raw.get("facts", {})
                last_reward_env = 0.0

        # ===== ä¸€æ¬¡ update ç»“æŸï¼šPPO æ›´æ–° =====
        T = len(rew_buf)
        if T == 0:
            continue

        rewards = np.array(rew_buf, dtype=np.float32)
        values = np.array(val_buf, dtype=np.float32)
        dones = np.array(done_buf, dtype=np.float32)

        # æœ«å€¼ bootstrap = 0ï¼ˆè¿™é‡Œç®€å•å¤„ç†ï¼‰
        values_ext = np.concatenate([values, np.array([0.0], dtype=np.float32)], axis=0)

        adv = compute_gae(rewards, values_ext, dones, gamma=gamma, lam=lam)
        returns = adv + values

        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        adv = (adv - adv.mean()) / (adv.std() + 1e-8)

        # è½¬ tensor
        obs_tensor = torch.from_numpy(np.array(obs_buf, dtype=np.float32)).to(DEVICE)
        act_tensor = torch.from_numpy(np.array(act_buf, dtype=np.int64)).to(DEVICE)
        logp_old_tensor = torch.from_numpy(np.array(logp_buf, dtype=np.float32)).to(DEVICE)
        adv_tensor = torch.from_numpy(adv).to(DEVICE)
        ret_tensor = torch.from_numpy(returns).to(DEVICE)

        # å¤š epoch æ‰“ä¹±è®­ç»ƒ
        num_samples = T
        idxs = np.arange(num_samples)

        for _ in range(ppo_epochs):
            np.random.shuffle(idxs)
            for start in range(0, num_samples, batch_size):
                end = start + batch_size
                batch_idx = idxs[start:end]

                b_obs = obs_tensor[batch_idx]
                b_act = act_tensor[batch_idx]
                b_logp_old = logp_old_tensor[batch_idx]
                b_adv = adv_tensor[batch_idx]
                b_ret = ret_tensor[batch_idx]

                logits, values_pred = ac(b_obs)
                dist = Categorical(logits=logits)
                logp = dist.log_prob(b_act)
                entropy = dist.entropy().mean()

                ratio = torch.exp(logp - b_logp_old)
                surr1 = ratio * b_adv
                surr2 = torch.clamp(ratio, 1.0 - clip_ratio, 1.0 + clip_ratio) * b_adv
                policy_loss = -torch.min(surr1, surr2).mean()

                value_loss = ((values_pred - b_ret) ** 2).mean()

                loss = policy_loss + value_coef * value_loss - entropy_coef * entropy

                optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(ac.parameters(), max_grad_norm)
                optimizer.step()

        # æ‰“å°æ—¶æ˜¾ç¤ºä¸¤ç§å¥–åŠ±
        print(
            f"[UPD {upd:03d}] steps={T:4d}  "
            f"Env_R={ep_reward_env:.3f}  Shaped_R={ep_reward_total:.3f}"
        )

        # ç®€å•ä¿å­˜ checkpointï¼ˆæ¯ 25 æ¬¡ updateï¼‰
        if upd % 25 == 0:
            ckpt_path = OUT_DIR / f"ac_upd{upd:03d}.pt"
            torch.save(
                {
                    "model": ac.state_dict(),
                    "optimizer": optimizer.state_dict(),
                    "update": upd,
                    "global_step": global_step,
                },
                ckpt_path,
            )
            print(f" ğŸ’¾ å·²ä¿å­˜ checkpoint: {ckpt_path}")

    if explain_log_f is not None:
        explain_log_f.close()
    print("âœ… è®­ç»ƒç»“æŸ")


if __name__ == "__main__":
    main()
